"""
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™:
- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç†
- sqlite-vecãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¥ç¶šç®¡ç†
- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
- è¨­å®šç®¡ç†
"""

import os
import sqlite3
import sqlite_vec
from typing import List, Dict, Any, Optional, Tuple
from transformers import AutoTokenizer, AutoModel
import torch
from dotenv import load_dotenv

# è¨­å®šã®èª­ã¿è¾¼ã¿
load_dotenv()

# å®šæ•°å®šç¾©
EMBEDDING_MODEL = "pfnet/plamo-embedding-1b"
SQLITE_DB_PATH = "search.db"
EMBEDDING_DIMENSION = 2048


class EmbeddingModelManager:
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.device = None
        self._is_loaded = False
        self._embedding_cache = {}  # åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
    def load_model(self) -> Tuple[AutoTokenizer, AutoModel, str]:
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
        if self._is_loaded:
            return self.tokenizer, self.model, self.device
        
        print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {EMBEDDING_MODEL}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            EMBEDDING_MODEL,
            trust_remote_code=True,
            revision="main"
        )
        self.model = AutoModel.from_pretrained(
            EMBEDDING_MODEL,
            trust_remote_code=True,
            revision="main"
        )
        
        # ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•åˆ¤å®š
        self.device = self._detect_device()
        self.model = self.model.to(self.device)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆæ¨è«–æœ€é©åŒ–ï¼‰
        self.model.eval()
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {self.device}")
        self._is_loaded = True
        
        return self.tokenizer, self.model, self.device
    
    def _detect_device(self) -> str:
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"ğŸš€ GPUä½¿ç”¨: {gpu_name} (CUDA {torch.version.cuda})")
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            print("ğŸš€ GPUä½¿ç”¨: Apple Metal Performance Shaders (MPS)")
            return "mps"
        else:
            print("âš ï¸  CPUä½¿ç”¨: GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return "cpu"
    
    def get_embedding(self, text: str) -> List[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if text in self._embedding_cache:
            return self._embedding_cache[text]
        
        if not self._is_loaded:
            self.load_model()
        
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True  # ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # æ¨è«–æœ€é©åŒ–ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            if self.device == "cuda":
                with torch.cuda.amp.autocast():
                    outputs = self.model(**inputs)
            else:
                outputs = self.model(**inputs)
            
            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ãŸã‚æœ€å¤§100ä»¶ï¼‰
        if len(self._embedding_cache) < 100:
            self._embedding_cache[text] = embeddings
        
        return embeddings


class SqliteVecDatabase:
    """sqlite-vecãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = SQLITE_DB_PATH):
        self.db_path = db_path
        self._connection = None
        self._connection_initialized = False
    
    def get_connection(self) -> sqlite3.Connection:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—ã™ã‚‹ï¼ˆæ¥ç¶šãƒ—ãƒ¼ãƒ«ä»˜ãï¼‰"""
        if not os.path.exists(self.db_path):
            raise RuntimeError(
                f"sqlite-vecãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.db_path}\n"
                "build_db.pyã‚’å®Ÿè¡Œã—ã¦DBã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚"
            )
        
        if not self._connection_initialized:
            self._connection = sqlite3.Connection(self.db_path)
            self._connection.enable_load_extension(True)
            sqlite_vec.load(self._connection)
            self._connection.enable_load_extension(False)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®šï¼ˆsqlite-vecãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«åŸºã¥ãï¼‰
            self._connection.execute("PRAGMA journal_mode=WAL")
            self._connection.execute("PRAGMA synchronous=NORMAL")
            self._connection.execute("PRAGMA cache_size=20000")  # ã‚ˆã‚Šå¤§ããªã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self._connection.execute("PRAGMA temp_store=MEMORY")
            self._connection.execute("PRAGMA page_size=4096")    # sqlite-vecãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä½¿ç”¨
            self._connection.execute("PRAGMA mmap_size=268435456")  # 256MB mmap
            self._connection.execute("PRAGMA optimize")  # ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ãƒŠãƒ¼æœ€é©åŒ–
            
            self._connection_initialized = True
        
        return self._connection
    
    def get_database_info(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
        with self.get_connection() as conn:
            # sqlite-vecãƒãƒ¼ã‚¸ãƒ§ãƒ³
            version = conn.execute("SELECT vec_version()").fetchone()[0]
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
            tables = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table'"
            ).fetchall()
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            doc_count = conn.execute("SELECT COUNT(*) FROM docs").fetchone()[0]
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°
            meta_count = conn.execute("SELECT COUNT(*) FROM doc_metadata").fetchone()[0]
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
            sample = conn.execute(
                "SELECT file_name, source FROM doc_metadata LIMIT 3"
            ).fetchall()
            
            return {
                "version": version,
                "table_count": len(tables),
                "doc_count": doc_count,
                "meta_count": meta_count,
                "sample_files": sample
            }
    
    def search_vectors(
        self,
        query_embedding: List[float],
        top_k: int = 5
    ) -> List[Tuple[str, str, str, str, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        query_blob = sqlite_vec.serialize_float32(query_embedding)
        
        conn = self.get_connection()
        cursor = conn.execute("""
            SELECT
                chunk_text,
                url,
                file_name,
                source,
                distance
            FROM docs
            WHERE embedding MATCH ?
              AND k = ?
            ORDER BY distance
        """, (query_blob, top_k))
        
        return cursor.fetchall()
    
    def close(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
        if self._connection:
            self._connection.close()
            self._connection = None
            self._connection_initialized = False


class VectorSearchService:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.model_manager = EmbeddingModelManager()
        self.database = SqliteVecDatabase()
        self._warmup_completed = False
    
    def _warmup(self):
        """åˆå›æ¤œç´¢ã®é«˜é€ŸåŒ–ã®ãŸã‚ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        if not self._warmup_completed:
            print("ğŸ”¥ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
            # ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰
            self.model_manager.load_model()
            # ãƒ€ãƒŸãƒ¼æ¤œç´¢ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æº–å‚™
            dummy_embedding = self.model_manager.get_embedding("test")
            self.database.search_vectors(dummy_embedding, 1)
            self._warmup_completed = True
            print("âœ… ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
    
    def search(self, query: str, top_k: int = 5, show_timing: bool = False) -> List[Dict[str, Any]]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹"""
        import time
        
        if not self._warmup_completed:
            self._warmup()
        
        start_time = time.time()
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        embedding_start = time.time()
        query_embedding = self.model_manager.get_embedding(query)
        embedding_time = time.time() - embedding_start
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
        search_start = time.time()
        results = self.database.search_vectors(query_embedding, top_k)
        search_time = time.time() - search_start
        
        total_time = time.time() - start_time
        
        if show_timing:
            cache_status = "HIT" if query in self.model_manager._embedding_cache else "MISS"
            print(f"â±ï¸  æ¤œç´¢æ™‚é–“è©³ç´°:")
            print(f"   ğŸ“¦ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {embedding_time:.3f}s (ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cache_status})")
            print(f"   ğŸ” DBæ¤œç´¢: {search_time:.3f}s")
            print(f"   ğŸ“Š åˆè¨ˆ: {total_time:.3f}s")
            print(f"   ğŸ¯ çµæœæ•°: {len(results)}ä»¶")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            if embedding_time > 0.1:
                print(f"   âš ï¸  åŸ‹ã‚è¾¼ã¿ç”ŸæˆãŒé…ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ ({embedding_time:.3f}s)")
            if search_time > 0.05:
                print(f"   âš ï¸  DBæ¤œç´¢ãŒé…ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ ({search_time:.3f}s)")
        
        # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        formatted_results = []
        for text, url, file_name, source, distance in results:
            formatted_results.append({
                "text": text,
                "url": url,
                "file": file_name,
                "source": source,
                "distance": distance
            })
        
        return formatted_results
    
    def get_database_info(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
        return self.database.get_database_info()
    
    def analyze_performance(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†ææƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
        conn = self.database.get_connection()
        
        # SQLiteçµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        stats = {}
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
        stats['db_size_mb'] = os.path.getsize(self.database.db_path) / (1024 * 1024)
        
        # PRAGMAæƒ…å ±
        pragma_info = {}
        pragmas = [
            'journal_mode', 'synchronous', 'cache_size',
            'temp_store', 'page_size', 'mmap_size'
        ]
        
        for pragma in pragmas:
            result = conn.execute(f"PRAGMA {pragma}").fetchone()
            pragma_info[pragma] = result[0] if result else None
        
        stats['pragma_settings'] = pragma_info
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
        stats['embedding_cache_size'] = len(self.model_manager._embedding_cache)
        stats['embedding_cache_limit'] = 100
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        stats['model_loaded'] = self.model_manager._is_loaded
        stats['device'] = self.model_manager.device if self.model_manager._is_loaded else None
        
        return stats


class ConfigManager:
    """è¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def get_data_source_config() -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šã‚’å–å¾—ã™ã‚‹"""
        return {
            "use_ftp_source": os.getenv("USE_FTP_SOURCE", "true").lower() == "true",
            "ftp_host": os.getenv("FTP_HOST", "192.168.7.48"),
            "ftp_user": os.getenv("FTP_USER", "anonymous"),
            "ftp_pass": os.getenv("FTP_PASS", ""),
            "ftp_data_dir": os.getenv("FTP_DATA_DIR", "/data"),
            "local_dir": os.getenv("LOCAL_DIR", "./data")
        }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
_vector_search_service = None

def get_vector_search_service() -> VectorSearchService:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹"""
    global _vector_search_service
    if _vector_search_service is None:
        _vector_search_service = VectorSearchService()
    return _vector_search_service